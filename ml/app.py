import logging
import os
import time
import joblib
import mysql.connector
import pandas as pd
from flask import Flask, jsonify, request
from mysql_config import DB_CONFIG
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# =================================================================
# 1. C·∫§U H√åNH C∆† B·∫¢N
# =================================================================

# C·∫•u h√¨nh logging ƒë·ªÉ hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt khi ch·∫°y
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
CACHE_FILE = 'recommendation_cache.pkl' # File ƒë·ªÉ l∆∞u cache model

# --- C√°c bi·∫øn to√†n c·ª•c ƒë·ªÉ l∆∞u tr·ªØ model v√† d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ---
df = None
tfidf = None
cosine_sim = None
movie_index = None


# =================================================================
# 2. C√ÅC H√ÄM X·ª¨ L√ù D·ªÆ LI·ªÜU V√Ä MODEL (CORE LOGIC)
# =================================================================

def load_data_and_model():
    """
    T·∫£i d·ªØ li·ªáu t·ª´ MySQL, x·ª≠ l√Ω, hu·∫•n luy·ªán model g·ª£i √Ω v√† l∆∞u v√†o cache.
    ƒê√¢y l√† h√†m t·ªën nhi·ªÅu t√†i nguy√™n nh·∫•t v√† s·∫Ω ƒë∆∞·ª£c g·ªçi khi kh·ªüi ƒë·ªông ho·∫∑c refresh.
    """
    logging.info("üîÑ B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫£i d·ªØ li·ªáu v√† hu·∫•n luy·ªán model...")
    
    # K·∫øt n·ªëi MySQL ƒë·ªÉ l·∫•y d·ªØ li·ªáu th√¥
    conn = mysql.connector.connect(**DB_CONFIG)
    
    # C√¢u l·ªánh SQL ƒë·ªÉ t·ªïng h·ª£p th√¥ng tin phim t·ª´ nhi·ªÅu b·∫£ng
    query = """
    SELECT 
        f.id,
        f.title,
        f.poster_url,
        f.release_year,
        GROUP_CONCAT(DISTINCT g.name) as genres,
        GROUP_CONCAT(DISTINCT a.name) as actors,
        d.name as director,
        c.name as country
    FROM film f
    LEFT JOIN movie_genres mg ON f.id = mg.movie_id
    LEFT JOIN genre g ON mg.genre_id = g.id
    LEFT JOIN movie_actors ma ON f.id = ma.movie_id
    LEFT JOIN actor a ON ma.actor_id = a.id
    LEFT JOIN director d ON f.director_id = d.id
    LEFT JOIN country c ON f.country_id = c.id
    GROUP BY f.id, f.title, d.name, c.name
    """
    
    temp_df = pd.read_sql(query, conn)
    conn.close()
    
    if temp_df.empty:
        raise Exception("Kh√¥ng c√≥ d·ªØ li·ªáu trong b·∫£ng 'film' ƒë·ªÉ hu·∫•n luy·ªán.")
    
    logging.info(f"‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng {len(temp_df)} phim t·ª´ database.")
    
    # H√†m helper ƒë·ªÉ k·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n th√†nh m·ªôt chu·ªói duy nh·∫•t
    def combine_features(row):
        # L·∫•y c√°c ƒë·∫∑c tr∆∞ng: th·ªÉ lo·∫°i, di·ªÖn vi√™n, ƒë·∫°o di·ªÖn, qu·ªëc gia
        features = [
            str(row.get('genres', '')),
            str(row.get('actors', '')),
            str(row.get('director', '')),
            str(row.get('country', ''))
        ]
        # N·ªëi ch√∫ng l·∫°i v√† chu·∫©n h√≥a (vi·∫øt th∆∞·ªùng, lo·∫°i b·ªè d·∫•u ph·∫©y)
        combined = ' '.join(filter(None, features)) # L·ªçc b·ªè c√°c gi√° tr·ªã None
        combined = combined.lower().replace(',', ' ').replace(';', ' ').replace(':', ' ')
        return ' '.join(combined.split()) # X√≥a c√°c kho·∫£ng tr·∫Øng th·ª´a
    
    # √Åp d·ª•ng h√†m tr√™n ƒë·ªÉ t·∫°o c·ªôt 'combined_features'
    temp_df['combined_features'] = temp_df.apply(combine_features, axis=1)
    logging.info("‚öôÔ∏è  ƒê√£ k·∫øt h·ª£p xong c√°c ƒë·∫∑c tr∆∞ng (features) cho m·ªói phim.")
    
    # Hu·∫•n luy·ªán model TF-IDF
    logging.info("üîÑ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán model TF-IDF...")
    temp_tfidf = TfidfVectorizer(
        max_features=2000,      # Ch·ªâ gi·ªØ l·∫°i 2000 t·ª´/c·ª•m t·ª´ ph·ªï bi·∫øn nh·∫•t
        ngram_range=(1, 2),     # Xem x√©t c·∫£ t·ª´ ƒë∆°n (unigram) v√† c·ª•m 2 t·ª´ (bigram)
        min_df=1,               # M·ªôt t·ª´ ph·∫£i xu·∫•t hi·ªán √≠t nh·∫•t 1 l·∫ßn
        max_df=0.95,            # Lo·∫°i b·ªè c√°c t·ª´ xu·∫•t hi·ªán trong h∆°n 95% t√†i li·ªáu (qu√° ph·ªï bi·∫øn)
        stop_words=None         # Kh√¥ng d√πng stop words c√≥ s·∫µn, v√¨ t√™n ri√™ng r·∫•t quan tr·ªçng
    )
    
    # T·∫°o ma tr·∫≠n TF-IDF: m·ªói h√†ng l√† m·ªôt phim, m·ªói c·ªôt l√† m·ªôt t·ª´/c·ª•m t·ª´
    tfidf_matrix = temp_tfidf.fit_transform(temp_df['combined_features'])
    logging.info(f"üìä Ma tr·∫≠n TF-IDF ƒë√£ ƒë∆∞·ª£c t·∫°o v·ªõi k√≠ch th∆∞·ªõc: {tfidf_matrix.shape}")
    
    # T√≠nh to√°n ma tr·∫≠n t∆∞∆°ng ƒë·ªìng Cosine
    # So s√°nh vector c·ªßa m·ªói phim v·ªõi t·∫•t c·∫£ c√°c phim kh√°c
    temp_cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
    logging.info(f"üìê Ma tr·∫≠n t∆∞∆°ng ƒë·ªìng Cosine ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n v·ªõi k√≠ch th∆∞·ªõc: {temp_cosine_sim.shape}")

    # T·∫°o m·ªôt t·ª´ ƒëi·ªÉn ƒë·ªÉ tra c·ª©u index c·ªßa phim t·ª´ ID
    temp_movie_index = dict(zip(temp_df['id'], temp_df.index))
    
    # ƒê√≥ng g√≥i t·∫•t c·∫£ v√†o m·ªôt ƒë·ªëi t∆∞·ª£ng ƒë·ªÉ l∆∞u cache
    cache_data = {
        'df': temp_df,
        'tfidf': temp_tfidf,
        'cosine_sim': temp_cosine_sim,
        'movie_index': temp_movie_index,
        'timestamp': time.time()
    }
    
    # L∆∞u cache xu·ªëng file
    joblib.dump(cache_data, CACHE_FILE)
    logging.info(f"‚úÖ ƒê√£ l∆∞u cache model v√† d·ªØ li·ªáu v√†o file '{CACHE_FILE}'")
    
    return cache_data

def _apply_cache_data(cache_data):
    """H√†m helper ƒë·ªÉ g√°n d·ªØ li·ªáu t·ª´ cache v√†o c√°c bi·∫øn to√†n c·ª•c."""
    global df, tfidf, cosine_sim, movie_index
    df = cache_data['df']
    tfidf = cache_data['tfidf']
    cosine_sim = cache_data['cosine_sim']
    movie_index = cache_data['movie_index']
    logging.info("‚úîÔ∏è  ƒê√£ √°p d·ª•ng d·ªØ li·ªáu t·ª´ cache v√†o c√°c bi·∫øn to√†n c·ª•c.")

def refresh_cache():
    """Refresh cache th·ªß c√¥ng, ƒë∆∞·ª£c g·ªçi b·ªüi webhook."""
    try:
        new_cache_data = load_data_and_model()
        _apply_cache_data(new_cache_data)
        return True
    except Exception:
        logging.exception("‚ùå L·ªói nghi√™m tr·ªçng khi ƒëang refresh cache.")
        return False

# =================================================================
# 3. KH·ªûI ƒê·ªòNG ·ª®NG D·ª§NG
# =================================================================
try:
    logging.info("üöÄ Kh·ªüi ƒë·ªông server, th·ª±c hi·ªán l√†m m·ªõi cache t·ª´ database...")
    initial_cache_data = load_data_and_model()
    _apply_cache_data(initial_cache_data)
    logging.info("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng nh·∫≠n request!")
except Exception:
    logging.exception("‚ùå L·ªói nghi√™m tr·ªçng khi kh·ªüi ƒë·ªông, kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu. Server s·∫Ω tho√°t.")
    exit(1)


# =================================================================
# 4. C√ÅC API ENDPOINTS
# =================================================================

@app.route('/recommend/content', methods=['GET'])
def content_based_recommend():
    """API g·ª£i √Ω c√°c phim li√™n quan (content-based) cho m·ªôt phim c·ª• th·ªÉ."""
    logging.info(f"‚ñ∂Ô∏è  Nh·∫≠n request GET /recommend/content v·ªõi params: {request.args}")
    try:
        movie_id = request.args.get('movie_id', type=int)
        if not movie_id or movie_id not in movie_index:
            logging.warning(f"‚ö†Ô∏è Y√™u c·∫ßu phim kh√¥ng h·ª£p l·ªá ho·∫∑c kh√¥ng t·ªìn t·∫°i: movie_id={movie_id}")
            return jsonify({"error": "Kh√¥ng t√¨m th·∫•y phim v·ªõi ID ƒë√£ cho"}), 404

        # L·∫•y index c·ªßa phim t·ª´ ID
        idx = movie_index[movie_id]
        
        # L·∫•y ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng c·ªßa phim n√†y v·ªõi t·∫•t c·∫£ c√°c phim kh√°c
        sim_scores = list(enumerate(cosine_sim[idx]))
        
        # S·∫Øp x·∫øp v√† l·∫•y 4 phim c√≥ ƒëi·ªÉm cao nh·∫•t (b·ªè qua phim ƒë·∫ßu ti√™n v√¨ ƒë√≥ l√† ch√≠nh n√≥)
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:5]
        
        movie_indices = [i[0] for i in sim_scores]
        recommendations = []
        
        for i in movie_indices:
            movie = df.iloc[i].to_dict()
            recommendations.append({
                'id': int(movie.get('id')),
                'title': movie.get('title'),
                'poster_url': movie.get('poster_url', ''),
                'release_year': movie.get('release_year', ''),
                'genres': movie.get('genres', ''),
                'similarity_score': float(sim_scores[movie_indices.index(i)][1])
            })
        
        logging.info(f"‚úÖ Tr·∫£ v·ªÅ {len(recommendations)} phim li√™n quan cho movie_id={movie_id}")
        return jsonify({
            "movie_id": movie_id,
            "type": "content-based",
            "recommended_movies": recommendations
        })
        
    except Exception:
        logging.exception(f"‚ùå L·ªói server khi x·ª≠ l√Ω /recommend/content")
        return jsonify({"error": "L·ªói h·ªá th·ªëng, vui l√≤ng th·ª≠ l·∫°i sau."}), 500

@app.route('/recommend/personalized', methods=['GET'])
def personalized_recommend():
    """API g·ª£i √Ω phim c√° nh√¢n h√≥a d·ª±a tr√™n l·ªãch s·ª≠ xem c·ªßa ng∆∞·ªùi d√πng."""
    logging.info(f"‚ñ∂Ô∏è  Nh·∫≠n request GET /recommend/personalized v·ªõi params: {request.args}")
    try:
        user_id = request.args.get('user_id', type=int)
        if not user_id:
            logging.warning("‚ö†Ô∏è Y√™u c·∫ßu thi·∫øu tham s·ªë 'user_id'")
            return jsonify({"error": "Thi·∫øu user_id"}), 400

        # L·∫•y l·ªãch s·ª≠ xem c·ªßa user t·ª´ DB
        conn = mysql.connector.connect(**DB_CONFIG)
        query = "SELECT f.id, f.title, f.poster_url, f.release_year, GROUP_CONCAT(DISTINCT g.name) as genres, GROUP_CONCAT(DISTINCT a.name) as actors, d.name as director, c.name as country FROM user_history uh JOIN film f ON uh.movie_id = f.id LEFT JOIN movie_genres mg ON f.id = mg.movie_id LEFT JOIN genre g ON mg.genre_id = g.id LEFT JOIN movie_actors ma ON f.id = ma.movie_id LEFT JOIN actor a ON ma.actor_id = a.id LEFT JOIN director d ON f.director_id = d.id LEFT JOIN country c ON f.country_id = c.id WHERE uh.user_id = %s GROUP BY f.id, f.title, d.name, c.name LIMIT 20"
        user_history = pd.read_sql(query, conn, params=(user_id,))
        conn.close()
        
        logging.info(f"üîé T√¨m th·∫•y {len(user_history)} phim trong l·ªãch s·ª≠ c·ªßa user_id={user_id}")

        # Tr∆∞·ªùng h·ª£p user ch∆∞a c√≥ l·ªãch s·ª≠ xem -> tr·∫£ v·ªÅ phim th·ªãnh h√†nh (fallback)
        if user_history.empty:
            logging.info(f"ü§∑‚Äç‚ôÇÔ∏è User_id={user_id} ch∆∞a c√≥ l·ªãch s·ª≠. S·ª≠ d·ª•ng fallback (phim th·ªãnh h√†nh).")
            sample_movies = df.head(4).to_dict('records')
            recommendations = []
            for movie in sample_movies:
                recommendations.append({
                    'id': int(movie.get('id')),
                    'title': movie.get('title'),
                    'poster_url': movie.get('poster_url', ''),
                    'release_year': movie.get('release_year', ''),
                    'genres': movie.get('genres', ''),
                    'similarity_score': 0.8 # ƒêi·ªÉm gi·∫£ l·∫≠p
                })
            return jsonify({
                "user_id": user_id,
                "type": "fallback",
                "recommended_movies": recommendations
            })

        # T·∫°o profile c·ªßa user b·∫±ng c√°ch k·∫øt h·ª£p ƒë·∫∑c tr∆∞ng c·ªßa c√°c phim ƒë√£ xem
        def combine_features(row):
            features = [ str(row.get(f, '')) for f in ['genres', 'actors', 'director', 'country'] ]
            combined = ' '.join(filter(None, features))
            return ' '.join(combined.lower().replace(',', ' ').split())
        
        user_history['combined_features'] = user_history.apply(combine_features, axis=1)
        user_profile = ' '.join(user_history['combined_features'].tolist())

        # Vector h√≥a profile user v√† t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng
        user_vector = tfidf.transform([user_profile])
        all_movie_vectors = tfidf.transform(df['combined_features'])
        similarity_scores = cosine_similarity(user_vector, all_movie_vectors).flatten()

        # L·ªçc ra c√°c phim user ƒë√£ xem v√† l·∫•y top 4 phim g·ª£i √Ω
        watched_movie_ids = set(user_history['id'].tolist())
        recommendations = []
        movie_scores = sorted(list(enumerate(similarity_scores)), key=lambda x: x[1], reverse=True)

        for idx, score in movie_scores:
            movie_id = int(df.iloc[idx]['id'])
            if movie_id not in watched_movie_ids and score > 0.01:
                movie = df.iloc[idx].to_dict()
                recommendations.append({
                    'id': movie_id,
                    'title': movie.get('title'),
                    'poster_url': movie.get('poster_url', ''),
                    'release_year': movie.get('release_year', ''),
                    'genres': movie.get('genres', ''),
                    'similarity_score': float(score)
                })
                if len(recommendations) >= 4:
                    break

        logging.info(f"‚úÖ Tr·∫£ v·ªÅ {len(recommendations)} phim c√° nh√¢n h√≥a cho user_id={user_id}")
        return jsonify({
            "user_id": user_id,
            "type": "personalized",
            "recommended_movies": recommendations
        })

    except Exception:
        logging.exception(f"‚ùå L·ªói server khi x·ª≠ l√Ω /recommend/personalized cho user_id={request.args.get('user_id')}")
        return jsonify({"error": "L·ªói h·ªá th·ªëng, vui l√≤ng th·ª≠ l·∫°i sau."}), 500


@app.route('/webhook/data-changed', methods=['POST'])
def webhook_data_changed():
    """
    Webhook chung, nh·∫≠n th√¥ng b√°o v·ªÅ b·∫•t k·ª≥ thay ƒë·ªïi d·ªØ li·ªáu n√†o
    (t·∫°o, s·ª≠a, x√≥a) v√† k√≠ch ho·∫°t vi·ªác l√†m m·ªõi cache.
    """
    try:
        logging.info("‚ñ∂Ô∏è  Nh·∫≠n request POST /webhook/data-changed")
        data = request.get_json()

        entity_id = data.get('entity_id') if data else "Kh√¥ng r√µ"
        event_type = data.get('event_type') if data else "Kh√¥ng r√µ"
        
        logging.info(f"üé¨ Webhook ƒë∆∞·ª£c k√≠ch ho·∫°t. S·ª± ki·ªán: '{event_type}', ID th·ª±c th·ªÉ: {entity_id}. B·∫Øt ƒë·∫ßu refresh cache.")
        
        if refresh_cache():
            return jsonify({
                "status": "success",
                "message": f"Cache ƒë√£ ƒë∆∞·ª£c refresh th√†nh c√¥ng do s·ª± ki·ªán '{event_type}' tr√™n th·ª±c th·ªÉ ID: {entity_id}"
            })
        else:
            return jsonify({
                "status": "error",
                "message": "L·ªói khi ƒëang refresh cache"
            }), 500
            
    except Exception as e:
        logging.exception("‚ùå L·ªói nghi√™m tr·ªçng khi x·ª≠ l√Ω webhook.")
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/health', methods=['GET'])
def health_check():
    """API ki·ªÉm tra tr·∫°ng th√°i c·ªßa h·ªá th·ªëng."""
    return jsonify({
        "status": "healthy",
        "movies_in_cache": len(df) if df is not None else 0,
        "tfidf_features": tfidf.max_features if tfidf is not None else 0,
        "cache_timestamp": time.ctime(os.path.getmtime(CACHE_FILE)) if os.path.exists(CACHE_FILE) else "N/A"
    })

if __name__ == '__main__':
    # Ch·∫°y server Flask ·ªü ch·∫ø ƒë·ªô debug
    logging.info("Flask server ƒëang kh·ªüi ƒë·ªông ·ªü ch·∫ø ƒë·ªô debug...")
    app.run(host='0.0.0.0', port=5000, debug=True)

